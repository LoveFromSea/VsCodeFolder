# 1.梯度下降
一个模型没有显性解
挑选一个初始值$w_0$，重复迭代$t=1,2,3$
$$W_t=W_{t-1}-\eta\frac{\partial l}{\partial W_{t-1}}$$
例如：爬山选最抖的方向最快下山
选择学习率不能太小（计算梯度太贵了，时间和计算成本），也不能太大

# 2.小批量随机梯度下降
最常用的版本——在深度学习内
随机采样b个样本$i_1,i_2,...,i_b$来近似损失
b是批量大小，另一个重要的超参数
|不能太小|不能太大|
|----|----|
|每次计算量太小，不适合并行来最大利用计算资源|内存消耗增加浪费计算，例如所有样本都是相同的情况|

# 3.总结
- 梯度下降通过不断沿着反梯度方向更行参数求解
- 小批量随机梯度下降是深度学习的默认求解算法
- 两个重要的超参数是批量大小和学习率
